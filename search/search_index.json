{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the MIScnn wiki! The open-source Python library MIScnn is an intuitive API allowing fast setup of medical image segmentation pipelines with state-of-the-art convolutional neural network and deep learning models in just a few lines of code. MIScnn provides several core features: - 2D/3D medical image segmentation for binary and multi-class problems - Data I/O, preprocessing and data augmentation for biomedical images - Patch-wise and full image analysis - State-of-the-art deep learning model and metric library - Intuitive and fast model utilization (training, prediction) - Multiple automatic evaluation techniques (e.g. cross-validation) - Custom model, data I/O, pre-/postprocessing and metric support - Based on Keras with Tensorflow as backend Philosophy of MIScnn User friendliness: MIScnn is an intuitive API designed for human beings, not machines. With a stronger growing interest in medical imaging processing, building medical image segmentation pipelines shouldn't be like inventing the wheel for every new user. MIScnn offers consistent & simple APIs for minimizing the number of user actions required for common use cases. Modularity: The general steps in medical image processing are identical in nearly all projects. Nevertheless, switching to another neural network architecture or data set format break most public available medical image processing software, today. MIScnn changes this situation! In particular, Data I/O, pre-/postprocessing functions, metrics and model architectures are standalone interfaces that you can easily switch. Easy extensibility: New interfaces are simple to integrate into the MIScnn pipeline. Existing interfaces provide sample examples. Additionally, MIScnn provide Abstract Base Classes for all interfaces which helps defining the structure and setup of custom interfaces. This results into easy integration of user architectures or adapting MIScnn to your data structure. Work with Python: MIScnn can be used in Python code, which is compact, easier to debug, allow easier deployment and integration into workflows. The utilization of an intuitive framework guide to better understanding and customization instead of standalone blackbox software. Pipeline visualization Support For help on how to use MIScnn, check out the tutorials, examples and further documentation. If you have any problem or identified a bug, please use the ISSUES system of GitHub. Share your problems and contribute to improve MIScnn. Why this name, MIScnn? Maybe, you are asking yourself: What abbreviation MIScnn stands for? The answer is quite simple: M edical I mage S egmentation with C onvolutional N eural N etworks and deep learning. Wondering what happened to the \"deep learning\" part and why it wasn't named \"MIScnndl\"? Also, a quite simple answer: No one would be able to pronounce it. ;) And how should I pronounce it correctly? Answer: MIZ-C-N-N","title":"Home"},{"location":"#welcome-to-the-miscnn-wiki","text":"The open-source Python library MIScnn is an intuitive API allowing fast setup of medical image segmentation pipelines with state-of-the-art convolutional neural network and deep learning models in just a few lines of code. MIScnn provides several core features: - 2D/3D medical image segmentation for binary and multi-class problems - Data I/O, preprocessing and data augmentation for biomedical images - Patch-wise and full image analysis - State-of-the-art deep learning model and metric library - Intuitive and fast model utilization (training, prediction) - Multiple automatic evaluation techniques (e.g. cross-validation) - Custom model, data I/O, pre-/postprocessing and metric support - Based on Keras with Tensorflow as backend","title":"Welcome to the MIScnn wiki!"},{"location":"#philosophy-of-miscnn","text":"User friendliness: MIScnn is an intuitive API designed for human beings, not machines. With a stronger growing interest in medical imaging processing, building medical image segmentation pipelines shouldn't be like inventing the wheel for every new user. MIScnn offers consistent & simple APIs for minimizing the number of user actions required for common use cases. Modularity: The general steps in medical image processing are identical in nearly all projects. Nevertheless, switching to another neural network architecture or data set format break most public available medical image processing software, today. MIScnn changes this situation! In particular, Data I/O, pre-/postprocessing functions, metrics and model architectures are standalone interfaces that you can easily switch. Easy extensibility: New interfaces are simple to integrate into the MIScnn pipeline. Existing interfaces provide sample examples. Additionally, MIScnn provide Abstract Base Classes for all interfaces which helps defining the structure and setup of custom interfaces. This results into easy integration of user architectures or adapting MIScnn to your data structure. Work with Python: MIScnn can be used in Python code, which is compact, easier to debug, allow easier deployment and integration into workflows. The utilization of an intuitive framework guide to better understanding and customization instead of standalone blackbox software.","title":"Philosophy of MIScnn"},{"location":"#pipeline-visualization","text":"","title":"Pipeline visualization"},{"location":"#support","text":"For help on how to use MIScnn, check out the tutorials, examples and further documentation. If you have any problem or identified a bug, please use the ISSUES system of GitHub. Share your problems and contribute to improve MIScnn.","title":"Support"},{"location":"#why-this-name-miscnn","text":"Maybe, you are asking yourself: What abbreviation MIScnn stands for? The answer is quite simple: M edical I mage S egmentation with C onvolutional N eural N etworks and deep learning. Wondering what happened to the \"deep learning\" part and why it wasn't named \"MIScnndl\"? Also, a quite simple answer: No one would be able to pronounce it. ;) And how should I pronounce it correctly? Answer: MIZ-C-N-N","title":"Why this name, MIScnn?"},{"location":"Architectures/","text":"The selection of a deep learning or convolutional neural network model is the most important step in a medical image segmentation pipeline. But there is a variety of model architectures and each has different strengths and weaknesses. MIScnn features an open model interface to load and switch between provided state-of-the-art convolutional neural network models like the popular U-Net model. Models are represented with the open-source neural network library Keras which provides an user-friendly API for commonly used neural-network building blocks on top of TensorFlow. The already implemented models are highly configurable by definable number of neurons, custom input sizes, optional dropout and batch normalization layers or enhanced architecture versions like the Optimized High Resolution Dense-U-Net model. Additionally, MIScnn offers architectures for 3D, as well as 2D medical image segmentation. Besides the flexibility in switching between already implemented models, the open model interface enables the ability for custom deep learning model implementations and simple integrating these custom models into the MIScnn pipeline. Usage of MIScnn Architectures An Architecture can be passed to the Neural Network class initialization. The Neural Network class automatically uses provided Architecture as model. # Import desired Architecture from miscnn.neural_network.architecture.unet.residual import Architecture # Initialize Architecture unet_residual = Architecture(activation=\"softmax\") # Pass Architecture to Neural Network Class model = Neural_Network(preprocessor=pp, architecture=unet_residual) Available Architectures provided by MIScnn Standard U-Net The popular and state-of-the-art architecture of medical image segmentation is the standard U-Net. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. Reference: Olaf Ronneberger, Philipp Fischer, Thomas Brox. 18 May 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI 2015 Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.standard import Architecture unet_standard = Architecture(n_filters=24, batch_normalization=False) model = Neural_Network(preprocessor=pp, architecture=unet_standard) Plain U-Net A plain variant of the popular U-Net architecture based on the winning model architecture of Fabian Isensee at the Kidney Tumor Segmentation Challenge 2019. Reference: http://arxiv.org/abs/1908.02182 Arguments: - activation : Activation function for the final output layer. Default 'softmax'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.plain import Architecture unet_plain = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_plain) Residual U-Net The Residual variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses an additional add layer after each convolutional block (2x conv layers). Be aware, that a residual architecture requires additional GPU RAM. Reference: Zhang Zhengxin, Liu Qingjie, Wang Yunhong. 2018. Road Extraction by Deep Residual U-Net. IEEE Geoscience and Remote Sensing Letters. Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.residual import Architecture unet_residual = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_residual) Compact U-Net The Compact variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses an additional concatenate layer after each convolutional block (2x conv layers). Be aware, that a compact architecture requires additional GPU RAM. Reference: https://arxiv.org/pdf/1512.03385.pdf Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.compact import Architecture unet_compact = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_compact) Dense U-Net The Dense variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses multiple concatenate layers in each convolutional block (2x conv layers). Be aware, that a dense architecture requires additional GPU RAM. Reference: https://arxiv.org/pdf/1512.03385.pdf Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.dense import Architecture unet_dense = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_dense) MultiRes U-Net The MultiRes variant of the popular U-Net architecture. It is an improved version of the standard U-Net and contains some small modifications to improve upon the already state-of-the-art U-Net model. Reference: Nabil Ibtehaz and M. Sohel Rahman. February 12, 2019. MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation. Neural Networks: Volume 121, January 2020, Pages 74-87 Arguments: - activation : Activation function for the final output layer. Default 'sigmoid'. Example: from miscnn.neural_network.architecture.unet.MultiRes import Architecture unet_multires = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_multires) Creation of custom Architectures todo todo Abstract Base Class for Architectures MIScnn also offers a documented Abstract Base Class for simple creation of custom Architectures for your specific needs. An Architecture inherits the abstract_architecture class with the following methods: create_model_2D, create_model_3D. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for an Architecture class # #-----------------------------------------------------# \"\"\" An abstract base class for a Architecture class. Methods: __init__ Object creation function create_model_2D: Creating a 2D Keras model create_model_3D: Creating a 3D Keras model \"\"\" class Abstract_Architecture(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the Architecture object creation. This function can be used to pass variables and options in the Architecture instance. The are no mandatory required parameters for the initialization. Parameter: None Return: None \"\"\" @abstractmethod def __init__(self): pass #---------------------------------------------# # Create 2D Model # #---------------------------------------------# \"\"\" Create the 2D version of a deep learning or convolutional neural network model. This function will be called inside the pipeline and have to return a functional Keras model for 2D images. The model itself should be created here or in a subfunction called by this function. It is possible to pass configurations through the initialization function of this class. Parameter: input_shape (Tuple): Input shape of the image data for the first model layer n_labels (Integer): Number of classes/labels of the segmentation (by default binary problem) Return: model (Keras model): A Keras model \"\"\" @abstractmethod def create_model_2D(self, input_shape, n_labels=2): pass #---------------------------------------------# # Create 3D Model # #---------------------------------------------# \"\"\" Create the 3D version of a deep learning or convolutional neural network model. This function will be called inside the pipeline and have to return a functional Keras model for 3D images. The model itself should be created here or in a subfunction called by this function. It is possible to pass configurations through the initialization function of this class. Parameter: input_shape (Tuple): Input shape of the image data for the first model layer n_labels (Integer): Number of classes/labels of the segmentation (by default binary problem) Return: model (Keras model): A Keras model \"\"\" @abstractmethod def create_model_3D(self, input_shape, n_labels=2): pass","title":"Architectures"},{"location":"Architectures/#usage-of-miscnn-architectures","text":"An Architecture can be passed to the Neural Network class initialization. The Neural Network class automatically uses provided Architecture as model. # Import desired Architecture from miscnn.neural_network.architecture.unet.residual import Architecture # Initialize Architecture unet_residual = Architecture(activation=\"softmax\") # Pass Architecture to Neural Network Class model = Neural_Network(preprocessor=pp, architecture=unet_residual)","title":"Usage of MIScnn Architectures"},{"location":"Architectures/#available-architectures-provided-by-miscnn","text":"","title":"Available Architectures provided by MIScnn"},{"location":"Architectures/#standard-u-net","text":"The popular and state-of-the-art architecture of medical image segmentation is the standard U-Net. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. Reference: Olaf Ronneberger, Philipp Fischer, Thomas Brox. 18 May 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI 2015 Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.standard import Architecture unet_standard = Architecture(n_filters=24, batch_normalization=False) model = Neural_Network(preprocessor=pp, architecture=unet_standard)","title":"Standard U-Net"},{"location":"Architectures/#plain-u-net","text":"A plain variant of the popular U-Net architecture based on the winning model architecture of Fabian Isensee at the Kidney Tumor Segmentation Challenge 2019. Reference: http://arxiv.org/abs/1908.02182 Arguments: - activation : Activation function for the final output layer. Default 'softmax'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.plain import Architecture unet_plain = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_plain)","title":"Plain U-Net"},{"location":"Architectures/#residual-u-net","text":"The Residual variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses an additional add layer after each convolutional block (2x conv layers). Be aware, that a residual architecture requires additional GPU RAM. Reference: Zhang Zhengxin, Liu Qingjie, Wang Yunhong. 2018. Road Extraction by Deep Residual U-Net. IEEE Geoscience and Remote Sensing Letters. Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.residual import Architecture unet_residual = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_residual)","title":"Residual U-Net"},{"location":"Architectures/#compact-u-net","text":"The Compact variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses an additional concatenate layer after each convolutional block (2x conv layers). Be aware, that a compact architecture requires additional GPU RAM. Reference: https://arxiv.org/pdf/1512.03385.pdf Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.compact import Architecture unet_compact = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_compact)","title":"Compact U-Net"},{"location":"Architectures/#dense-u-net","text":"The Dense variant of the popular U-Net architecture. It is an improved version of the standard U-Net and uses multiple concatenate layers in each convolutional block (2x conv layers). Be aware, that a dense architecture requires additional GPU RAM. Reference: https://arxiv.org/pdf/1512.03385.pdf Arguments: - n_filters : Number of filters in the first layer. Default 32. - depth : Number of layers of the U-Net structure. Default 4. - activation : Activation function for the final output layer. Default 'sigmoid'. - batch_normalization : Boolean option, whether batch normalization should be applied or not. Default True. Example: from miscnn.neural_network.architecture.unet.dense import Architecture unet_dense = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_dense)","title":"Dense U-Net"},{"location":"Architectures/#multires-u-net","text":"The MultiRes variant of the popular U-Net architecture. It is an improved version of the standard U-Net and contains some small modifications to improve upon the already state-of-the-art U-Net model. Reference: Nabil Ibtehaz and M. Sohel Rahman. February 12, 2019. MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation. Neural Networks: Volume 121, January 2020, Pages 74-87 Arguments: - activation : Activation function for the final output layer. Default 'sigmoid'. Example: from miscnn.neural_network.architecture.unet.MultiRes import Architecture unet_multires = Architecture() model = Neural_Network(preprocessor=pp, architecture=unet_multires)","title":"MultiRes U-Net"},{"location":"Architectures/#creation-of-custom-architectures","text":"todo todo","title":"Creation of custom Architectures"},{"location":"Architectures/#abstract-base-class-for-architectures","text":"MIScnn also offers a documented Abstract Base Class for simple creation of custom Architectures for your specific needs. An Architecture inherits the abstract_architecture class with the following methods: create_model_2D, create_model_3D. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for an Architecture class # #-----------------------------------------------------# \"\"\" An abstract base class for a Architecture class. Methods: __init__ Object creation function create_model_2D: Creating a 2D Keras model create_model_3D: Creating a 3D Keras model \"\"\" class Abstract_Architecture(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the Architecture object creation. This function can be used to pass variables and options in the Architecture instance. The are no mandatory required parameters for the initialization. Parameter: None Return: None \"\"\" @abstractmethod def __init__(self): pass #---------------------------------------------# # Create 2D Model # #---------------------------------------------# \"\"\" Create the 2D version of a deep learning or convolutional neural network model. This function will be called inside the pipeline and have to return a functional Keras model for 2D images. The model itself should be created here or in a subfunction called by this function. It is possible to pass configurations through the initialization function of this class. Parameter: input_shape (Tuple): Input shape of the image data for the first model layer n_labels (Integer): Number of classes/labels of the segmentation (by default binary problem) Return: model (Keras model): A Keras model \"\"\" @abstractmethod def create_model_2D(self, input_shape, n_labels=2): pass #---------------------------------------------# # Create 3D Model # #---------------------------------------------# \"\"\" Create the 3D version of a deep learning or convolutional neural network model. This function will be called inside the pipeline and have to return a functional Keras model for 3D images. The model itself should be created here or in a subfunction called by this function. It is possible to pass configurations through the initialization function of this class. Parameter: input_shape (Tuple): Input shape of the image data for the first model layer n_labels (Integer): Number of classes/labels of the segmentation (by default binary problem) Return: model (Keras model): A Keras model \"\"\" @abstractmethod def create_model_3D(self, input_shape, n_labels=2): pass","title":"Abstract Base Class for Architectures"},{"location":"Contribute/","text":"This wiki is currently under construction! Sadly, this page isn't finish, yet :(","title":"How to Contribute"},{"location":"Credits/","text":"Contact Dominik M\u00fcller Email: dominik.mueller@informatik.uni-augsburg.de IT-Infrastructure for Translational Medical Research University Augsburg Bavaria, Germany How to cite / Author Dominik M\u00fcller and Frank Kramer. (2019) MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning. arXiv e-print: https://arxiv.org/abs/1910.09308 Article{miscnn, title={MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}, author={Dominik M\u00fcller and Frank Kramer}, year={2019}, eprint={1910.09308}, archivePrefix={arXiv}, primaryClass={eess.IV} } Thank you for citing our work. License This project is licensed under the GNU GENERAL PUBLIC LICENSE Version 3. See the LICENSE.md file for license rights and limitations.","title":"Author and Contacts"},{"location":"Credits/#contact","text":"Dominik M\u00fcller Email: dominik.mueller@informatik.uni-augsburg.de IT-Infrastructure for Translational Medical Research University Augsburg Bavaria, Germany","title":"Contact"},{"location":"Credits/#how-to-cite-author","text":"Dominik M\u00fcller and Frank Kramer. (2019) MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning. arXiv e-print: https://arxiv.org/abs/1910.09308 Article{miscnn, title={MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}, author={Dominik M\u00fcller and Frank Kramer}, year={2019}, eprint={1910.09308}, archivePrefix={arXiv}, primaryClass={eess.IV} } Thank you for citing our work.","title":"How to cite / Author"},{"location":"Credits/#license","text":"This project is licensed under the GNU GENERAL PUBLIC LICENSE Version 3. See the LICENSE.md file for license rights and limitations.","title":"License"},{"location":"Data-Augmentation/","text":"Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. The point of data augmentation is, that the model will learn meaningful patterns instead of meaningless characteristics due to a small data set size. The Data Augmentation class is based on python library: batchgenerators by MIC@DKFZ. Reference: https://github.com/MIC-DKFZ/batchgenerators Methods Initialization Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=False, mirror=False, brightness=True, contrast=True, gamma=True, gaussian_noise=True) Initialization function for creating an Data Augmentation object. Arguments: - cycles: Number of augmentated image copies that should be created. - scaling: Boolean, whether scaling should be performed as data augmentation. - rotations: Boolean, whether rotations should be performed as data augmentation. - elastic_deform: Boolean, whether elastic deformation should be performed as data augmentation. - mirror: Boolean, whether mirroring should be performed as data augmentation. - brightness: Boolean, whether brightness changes should be added as data augmentation. - contrast: Boolean, whether contrast changes should be added as data augmentation. - gamma: Boolean, whether gamma changes should be added as data augmentation. - gaussian_noise: Boolean, whether Gaussian noise should be added as data augmentation. Returns: A Data_Augmentation class object. Have to be passed to the Preprocessor class. Configurable Class Variables: Various configurations of specific data augmentation techniques can be adjusted if needed. The documentation of the augmentation functions can be found in the official git repository of batchgenerators: https://github.com/MIC-DKFZ/batchgenerators The default setttings were obtained from nnUNet. An U-Net implementation of the batchgenerators authors: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/training/data_augmentation/default_data_augmentation.py config_p_per_sample = 0.15 config_mirror_axes = (0, 1, 2) config_contrast_range = (0.3, 3.0) config_brightness_range = (0.5, 2) config_gamma_range = (0.7, 1.5) config_gaussian_noise_range = (0.0, 0.05) config_elastic_deform_alpha = (0.0, 900.0) config_elastic_deform_sigma = (9.0, 13.0) config_rotations_angleX = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_rotations_angleY = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_rotations_angleZ = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_scaling_range = (0.85, 1.25) Example: # Initialize data_aug = Data_Augmentation(cycles=2, scaling=False, rotations=False, elastic_deform=False, mirror=False, brightness=False, contrast=False, gamma=True, gaussian_noise=True) # Further configurations data_aug.config_p_per_sample = 0.35 data_aug.config_contrast_range = (1, 2) # Pass to Processor pp = Preprocessor(data_io, data_aug=data_aug) run run(img_data, seg_data) Run data augmentation on a given image and segmentation NumPy array. This function is automatically called inside the Preprocessor object during batch generation. Therefore, the user do not have to use this function on their own. Arguments: - img_data: NumPy array containing the image - seg_data: NumPy array containing the segmentation Returns: Two NumPy arrays. One NumPy array containing the augmentated image and one NumPy array containing the corresponding augmentated segmentation. Example: img_data_aug, seg_data_aug = data_aug.run(img_data, seg_data)","title":"Data Augmentation"},{"location":"Data-Augmentation/#methods","text":"","title":"Methods"},{"location":"Data-Augmentation/#initialization","text":"Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=False, mirror=False, brightness=True, contrast=True, gamma=True, gaussian_noise=True) Initialization function for creating an Data Augmentation object. Arguments: - cycles: Number of augmentated image copies that should be created. - scaling: Boolean, whether scaling should be performed as data augmentation. - rotations: Boolean, whether rotations should be performed as data augmentation. - elastic_deform: Boolean, whether elastic deformation should be performed as data augmentation. - mirror: Boolean, whether mirroring should be performed as data augmentation. - brightness: Boolean, whether brightness changes should be added as data augmentation. - contrast: Boolean, whether contrast changes should be added as data augmentation. - gamma: Boolean, whether gamma changes should be added as data augmentation. - gaussian_noise: Boolean, whether Gaussian noise should be added as data augmentation. Returns: A Data_Augmentation class object. Have to be passed to the Preprocessor class. Configurable Class Variables: Various configurations of specific data augmentation techniques can be adjusted if needed. The documentation of the augmentation functions can be found in the official git repository of batchgenerators: https://github.com/MIC-DKFZ/batchgenerators The default setttings were obtained from nnUNet. An U-Net implementation of the batchgenerators authors: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/training/data_augmentation/default_data_augmentation.py config_p_per_sample = 0.15 config_mirror_axes = (0, 1, 2) config_contrast_range = (0.3, 3.0) config_brightness_range = (0.5, 2) config_gamma_range = (0.7, 1.5) config_gaussian_noise_range = (0.0, 0.05) config_elastic_deform_alpha = (0.0, 900.0) config_elastic_deform_sigma = (9.0, 13.0) config_rotations_angleX = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_rotations_angleY = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_rotations_angleZ = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi) config_scaling_range = (0.85, 1.25) Example: # Initialize data_aug = Data_Augmentation(cycles=2, scaling=False, rotations=False, elastic_deform=False, mirror=False, brightness=False, contrast=False, gamma=True, gaussian_noise=True) # Further configurations data_aug.config_p_per_sample = 0.35 data_aug.config_contrast_range = (1, 2) # Pass to Processor pp = Preprocessor(data_io, data_aug=data_aug)","title":"Initialization"},{"location":"Data-Augmentation/#run","text":"run(img_data, seg_data) Run data augmentation on a given image and segmentation NumPy array. This function is automatically called inside the Preprocessor object during batch generation. Therefore, the user do not have to use this function on their own. Arguments: - img_data: NumPy array containing the image - seg_data: NumPy array containing the segmentation Returns: Two NumPy arrays. One NumPy array containing the augmentated image and one NumPy array containing the corresponding augmentated segmentation. Example: img_data_aug, seg_data_aug = data_aug.run(img_data, seg_data)","title":"run"},{"location":"Data-IO/","text":"This class provides functionality for handling all input and output processes of the imaging data, as well as the temporary backup of batches to the disk. The Data I/O class is the gateway into the MIScnn pipeline and therefore one of the four core classes in MIScnn. The user is only required to create an instance of the Data IO class with the desired specifications and IO interface for the correct format. It is possible to create a custom IO interface for handling special data structures or formats. Methods Initialization Data_IO(interface, input_path, output_path=\"predictions\", batch_path=\"batches\", delete_batchDir=True) Initialization function for creating an object of the Data IO class. Arguments: - interface: Data I/O interface object. - input_path: Path to the input image data directory (passed to Data I/O interface object). - output_path: Path to the output directory for predictions (passed to Data I/O interface object). - batch_path: Path to the directory for temporary files like batches or preprocessing files. - delete_batchDir: Boolean, whether the temporary batch directory should be deleted, afterwards. Returns: A Data_IO class object. Have to be passed to the Preprocessor class and can be used to access the sample list. Example: # Create a Data I/O interface for kidney tumor CT scans in NIfTI format from miscnn.data_loading.interfaces import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) # Initialize data path and create the Data I/O instance data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data.original/\" data_io = miscnn.Data_IO(interface, data_path) get_indiceslist get_indiceslist() Return a list of indices for all available samples Arguments: None Returns: List of indices (Strings) Example: sample_list = data_io.get_indiceslist() print(sample_list[0:3]) # ['case_00000', 'case_00001', 'case_00002'] sample_loader sample_loader(index, load_seg=True, load_pred=False, backup=False) Load a sample class object given a valid sample index. Arguments: - index: Index (String) of a sample. - load_seg: Boolean, whether the segmentation should be loaded, as well. - load_pred: Boolean, whether the prediction should be loaded, as well. - backup: Boolean, whether the sample file should be created from an original image or loaded from a temporary backup file. Returns: A sample class object. Example: index = \"case_00000\" sample = data_io.sample_loader(index, load_seg=True) save_prediction save_prediction(pred, index) Save a segmentation prediction into a file. Arguments: - pred: NumPy array containing a prediction. - index: Index (String) of a sample. Returns: None Example: index = \"case_00000\" prediction = model.predict([index], direct_output=True)[0] data_io.save_prediction(prediction, index)","title":"Data I/O"},{"location":"Data-IO/#methods","text":"","title":"Methods"},{"location":"Data-IO/#initialization","text":"Data_IO(interface, input_path, output_path=\"predictions\", batch_path=\"batches\", delete_batchDir=True) Initialization function for creating an object of the Data IO class. Arguments: - interface: Data I/O interface object. - input_path: Path to the input image data directory (passed to Data I/O interface object). - output_path: Path to the output directory for predictions (passed to Data I/O interface object). - batch_path: Path to the directory for temporary files like batches or preprocessing files. - delete_batchDir: Boolean, whether the temporary batch directory should be deleted, afterwards. Returns: A Data_IO class object. Have to be passed to the Preprocessor class and can be used to access the sample list. Example: # Create a Data I/O interface for kidney tumor CT scans in NIfTI format from miscnn.data_loading.interfaces import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) # Initialize data path and create the Data I/O instance data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data.original/\" data_io = miscnn.Data_IO(interface, data_path)","title":"Initialization"},{"location":"Data-IO/#get_indiceslist","text":"get_indiceslist() Return a list of indices for all available samples Arguments: None Returns: List of indices (Strings) Example: sample_list = data_io.get_indiceslist() print(sample_list[0:3]) # ['case_00000', 'case_00001', 'case_00002']","title":"get_indiceslist"},{"location":"Data-IO/#sample_loader","text":"sample_loader(index, load_seg=True, load_pred=False, backup=False) Load a sample class object given a valid sample index. Arguments: - index: Index (String) of a sample. - load_seg: Boolean, whether the segmentation should be loaded, as well. - load_pred: Boolean, whether the prediction should be loaded, as well. - backup: Boolean, whether the sample file should be created from an original image or loaded from a temporary backup file. Returns: A sample class object. Example: index = \"case_00000\" sample = data_io.sample_loader(index, load_seg=True)","title":"sample_loader"},{"location":"Data-IO/#save_prediction","text":"save_prediction(pred, index) Save a segmentation prediction into a file. Arguments: - pred: NumPy array containing a prediction. - index: Index (String) of a sample. Returns: None Example: index = \"case_00000\" prediction = model.predict([index], direct_output=True)[0] data_io.save_prediction(prediction, index)","title":"save_prediction"},{"location":"Examples/","text":"The examples for MIScnn are implemented in Jupyter Notebooks. Jupyter Notebooks offer reproducibility by including the output of each coding block, but can also integrate commentary blocks with Markdown. Also, Jupyter Notebooks can be directly displayed in GitHub without any additional software. Overview Data Set Task Jupyter Notebook KiTS19 3-fold cross-validation KiTS19.ipynb Kidney Tumor Segmentation Challenge 2019 (KiTS19) With more than 400 000 kidney cancer diagnoses worldwide in 2018, kidney cancer is under the top 10 most common cancer types in men and under the top 15 in woman. The goal of the KiTS19 challenge is the development of reliable and unbiased kidney and kidney tumor semantic segmentation methods. Therefore, the challenge built a data set for arterial phase abdominal CT scan of 300 kidney cancer patients. The original scans have an image resolution of 512x512 and on average 216 slices (highest slice number is 1059). For all CT scans, a ground truth semantic segmentation was created by experts. This semantic segmentation labeled each pixel with one of three classes: Background, kidney or tumor. 210 of these CT scans with the ground truth segmentation were published during the training phase of the challenge, whereas 90 CT scans without published ground truth were released afterwards in the submission phase. The CT scans were provided in NIfTI format in original resolution and also in interpolated resolution with slice thickness normalization. The data for the KITS19 challenge can be found here: https://github.com/neheller/kits19","title":"Examples"},{"location":"Examples/#overview","text":"Data Set Task Jupyter Notebook KiTS19 3-fold cross-validation KiTS19.ipynb","title":"Overview"},{"location":"Home/","text":"Welcome to the MIScnn wiki! The open-source Python library MIScnn is an intuitive API allowing fast setup of medical image segmentation pipelines with state-of-the-art convolutional neural network and deep learning models in just a few lines of code. MIScnn provides several core features: - 2D/3D medical image segmentation for binary and multi-class problems - Data I/O, preprocessing and data augmentation for biomedical images - Patch-wise and full image analysis - State-of-the-art deep learning model and metric library - Intuitive and fast model utilization (training, prediction) - Multiple automatic evaluation techniques (e.g. cross-validation) - Custom model, data I/O, pre-/postprocessing and metric support - Based on Keras with Tensorflow as backend Philosophy of MIScnn User friendliness: MIScnn is an intuitive API designed for human beings, not machines. With a stronger growing interest in medical imaging processing, building medical image segmentation pipelines shouldn't be like inventing the wheel for every new user. MIScnn offers consistent & simple APIs for minimizing the number of user actions required for common use cases. Modularity: The general steps in medical image processing are identical in nearly all projects. Nevertheless, switching to another neural network architecture or data set format break most public available medical image processing software, today. MIScnn changes this situation! In particular, Data I/O, pre-/postprocessing functions, metrics and model architectures are standalone interfaces that you can easily switch. Easy extensibility: New interfaces are simple to integrate into the MIScnn pipeline. Existing interfaces provide sample examples. Additionally, MIScnn provide Abstract Base Classes for all interfaces which helps defining the structure and setup of custom interfaces. This results into easy integration of user architectures or adapting MIScnn to your data structure. Work with Python: MIScnn can be used in Python code, which is compact, easier to debug, allow easier deployment and integration into workflows. The utilization of an intuitive framework guide to better understanding and customization instead of standalone blackbox software. Pipeline visualization Support For help on how to use MIScnn, check out the tutorials, examples and further documentation. If you have any problem or identified a bug, please use the ISSUES system of GitHub. Share your problems and contribute to improve MIScnn. Why this name, MIScnn? Maybe, you are asking yourself: What abbreviation MIScnn stands for? The answer is quite simple: M edical I mage S egmentation with C onvolutional N eural N etworks and deep learning. Wondering what happened to the \"deep learning\" part and why it wasn't named \"MIScnndl\"? Also, a quite simple answer: No one would be able to pronounce it. ;) And how should I pronounce it correctly? Answer: MIZ-C-N-N","title":"What is MIScnn?"},{"location":"Home/#welcome-to-the-miscnn-wiki","text":"The open-source Python library MIScnn is an intuitive API allowing fast setup of medical image segmentation pipelines with state-of-the-art convolutional neural network and deep learning models in just a few lines of code. MIScnn provides several core features: - 2D/3D medical image segmentation for binary and multi-class problems - Data I/O, preprocessing and data augmentation for biomedical images - Patch-wise and full image analysis - State-of-the-art deep learning model and metric library - Intuitive and fast model utilization (training, prediction) - Multiple automatic evaluation techniques (e.g. cross-validation) - Custom model, data I/O, pre-/postprocessing and metric support - Based on Keras with Tensorflow as backend","title":"Welcome to the MIScnn wiki!"},{"location":"Home/#philosophy-of-miscnn","text":"User friendliness: MIScnn is an intuitive API designed for human beings, not machines. With a stronger growing interest in medical imaging processing, building medical image segmentation pipelines shouldn't be like inventing the wheel for every new user. MIScnn offers consistent & simple APIs for minimizing the number of user actions required for common use cases. Modularity: The general steps in medical image processing are identical in nearly all projects. Nevertheless, switching to another neural network architecture or data set format break most public available medical image processing software, today. MIScnn changes this situation! In particular, Data I/O, pre-/postprocessing functions, metrics and model architectures are standalone interfaces that you can easily switch. Easy extensibility: New interfaces are simple to integrate into the MIScnn pipeline. Existing interfaces provide sample examples. Additionally, MIScnn provide Abstract Base Classes for all interfaces which helps defining the structure and setup of custom interfaces. This results into easy integration of user architectures or adapting MIScnn to your data structure. Work with Python: MIScnn can be used in Python code, which is compact, easier to debug, allow easier deployment and integration into workflows. The utilization of an intuitive framework guide to better understanding and customization instead of standalone blackbox software.","title":"Philosophy of MIScnn"},{"location":"Home/#pipeline-visualization","text":"","title":"Pipeline visualization"},{"location":"Home/#support","text":"For help on how to use MIScnn, check out the tutorials, examples and further documentation. If you have any problem or identified a bug, please use the ISSUES system of GitHub. Share your problems and contribute to improve MIScnn.","title":"Support"},{"location":"Home/#why-this-name-miscnn","text":"Maybe, you are asking yourself: What abbreviation MIScnn stands for? The answer is quite simple: M edical I mage S egmentation with C onvolutional N eural N etworks and deep learning. Wondering what happened to the \"deep learning\" part and why it wasn't named \"MIScnndl\"? Also, a quite simple answer: No one would be able to pronounce it. ;) And how should I pronounce it correctly? Answer: MIZ-C-N-N","title":"Why this name, MIScnn?"},{"location":"IO-Interfaces/","text":"The Data IO class provides functionality for handling all input and output processes of the imaging data, as well as the temporary backup of batches to the disk. Due to the large variety of different imaging formats, the user is required to provide an Data IO Interface for the correct format. The aim of the IO Interface is to handle the loading and saving process of images in their specific formats and file structure. The Data IO Interface module enables MIScnn to handle a wide variety of imaging formats, as well as offers the possibility to integrate any user-required image format or data structure into the MIScnn pipeline. Usage of MIScnn Data IO Interfaces The Data IO Interface can be passed to the Data IO class initialization. The Data IO class automatically uses the IO Interface for image loading and saving. # Import desired Data IO Interface from data_loading.interfaces.nifti_io import NIFTI_interface # Initialize Data IO Interface interface = NIFTI_interface(pattern=\"case_00[0-9]+\", channels=1, classes=3) # Pass Data IO Interface to Data IO class from data_loading.data_io import Data_IO data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data/\" data_io = Data_IO(interface, data_path) Available Data IO Interfaces provided by MIScnn NIfTI IO Interface Data I/O Interface for NIfTI files. The Neuroimaging Informatics Technology Initiative file format is designed to contain brain images from e.g. magnetic resonance tomography. Nevertheless, it is currently broadly used for any 3D medical image data. # Import NIfTI IO Interface and initialize it from data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_00[0-9]+\", channels=1, classes=3) # Pass Interface to Data IO class data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data.interpolated/\" data_io = Data_IO(interface, data_path) Image IO Interface Coming soon for PNG, JPEG and TIF :) DICOM IO Interface Coming soon for DICOM data :) Dictionary IO Interface Data I/O Interface for python dictionaries. This interface uses the basic-python dictionary to load and save data. Therefore the complete data management happens in the memory. Therefore, for common data set sizes this is NOT recommended! It is advised to use already provided I/O interfaces of this package or to implement a custom I/O interface for perfect usability. Dictionary structure: Key: sample_index Value: Tuple containing: (0) image as numpy array (1) optional segmentation as numpy array (2) optional prediction as numpy array (3) optional details Example: # Create dictionary containing the data set dict = {\"case_00001\": (image1, segmentation1), \"case_00002\": (image2, segmentation2)} # Import Data IO Interface and pass dictionary to it from data_loading.interfaces import Dictionary_interface interface = Dictionary_interface(dict, channels=1, classes=2, three_dim=True) # Pass Interface to Data IO class data_path = \"/home/mudomini/MIScnn_tmp/\" data_io = Data_IO(interface, data_path) Creation of custom Data IO Interfaces todo todo Abstract Base Class for Data IO Interfaces MIScnn also offers a documented Abstract Base Class for simple creation of custom Data IO Interfaces for your specific needs. A Data IO Interface inherits the abstract_io class with the following methods: initialize, load_image, load_segmentation, load_prediction, save_prediction, load_details. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for the Data IO class # #-----------------------------------------------------# \"\"\" An abstract base class for a Data_IO interface. Methods: __init__ Object creation function initialize: Prepare the data set and create indices list load_image: Load an image load_segmentation: Load a segmentation load_prediction: Load a prediction from file load_details: Load optional information save_prediction: Save a prediction to file \"\"\" class Abstract_IO(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the I/O interface object creation. This function can be used to pass variables in the custom I/O interface. The only required passed variable is the number of channels in the images, the number of classes in the segmentation and the dimension of the data. Parameter: channels (integer): Number of channels of the image (grayscale:1, RGB:3) classes (integer): Number of classes in the segmentation (binary:2, multi-class:3+) three_dim (boolean): Variable to express, if the data is two or three dimensional Return: None \"\"\" @abstractmethod def __init__(self, channels=1, classes=2, three_dim=False): self.channels = channels self.classes = classes self.three_dim = three_dim pass #---------------------------------------------# # initialize # #---------------------------------------------# \"\"\" Initialize and prepare the image data set, return the number of samples in the data set Parameter: input_path (string): Path to the input data directory, in which all imaging data have to be accessible Return: indices_list [list]: List of indices. The Data_IO class will iterate over this list and call the load_image and load_segmentation functions providing the current index. This can be used to train/predict on just a subset of the data set. e.g. indices_list = [0,1,9] -> load_image(0) | load_image(1) | load_image(9) \"\"\" @abstractmethod def initialize(self, input_path): pass #---------------------------------------------# # load_image # #---------------------------------------------# \"\"\" Load the image with the index i from the data set and return it as a numpy matrix. Be aware that MIScnn only supports a last_channel structure. 2D: (x,y,channel) or (x,y) 3D: (x,y,z,channel) or (x,y,z) Parameter: index (variable): An index from the provided indices_list of the initialize function Return: image [numpy matrix]: A numpy matrix/array containing the image \"\"\" @abstractmethod def load_image(self, i): pass #---------------------------------------------# # load_segmentation # #---------------------------------------------# \"\"\" Load the segmentation of the image with the index i from the data set and return it as a numpy matrix. Be aware that MIScnn only supports a last_channel structure. 2D: (x,y,channel) or (x,y) 3D: (x,y,z,channel) or (x,y,z) Parameter: index (variable): An index from the provided indices_list of the initialize function Return: seg [numpy matrix]: A numpy matrix/array containing the segmentation \"\"\" @abstractmethod def load_segmentation(self, i): pass #---------------------------------------------# # load_prediction # #---------------------------------------------# \"\"\" Load the prediction of the image with the index i from the output directory and return it as a numpy matrix. Parameter: index (variable): An index from the provided indices_list of the initialize function output_path (string): Path to the output directory in which MIScnn predictions are stored. Return: pred [numpy matrix]: A numpy matrix/array containing the prediction \"\"\" @abstractmethod def load_prediction(self, i, output_path): pass #---------------------------------------------# # load_details # #---------------------------------------------# \"\"\" Load optional details during sample creation. This function can be used to parse whatever information you want into the sample object. This enables usage of these information in custom preprocessing subfunctions. Example: Slice thickness / voxel spacing Parameter: index (variable): An index from the provided indices_list of the initialize function Return: dict [dictionary]: A basic Python dictionary \"\"\" @abstractmethod def load_details(self, i): pass #---------------------------------------------# # save_prediction # #---------------------------------------------# \"\"\" Backup the prediction of the image with the index i into the output directory. Parameter: pred (numpy matrix): MIScnn computed prediction for the sample index index (variable): An index from the provided indices_list of the initialize function output_path (string): Path to the output directory in which MIScnn predictions are stored. This directory will be created if not existent Return: None \"\"\" @abstractmethod def save_prediction(self, pred, i, output_path): pass","title":"I/O Interfaces"},{"location":"IO-Interfaces/#usage-of-miscnn-data-io-interfaces","text":"The Data IO Interface can be passed to the Data IO class initialization. The Data IO class automatically uses the IO Interface for image loading and saving. # Import desired Data IO Interface from data_loading.interfaces.nifti_io import NIFTI_interface # Initialize Data IO Interface interface = NIFTI_interface(pattern=\"case_00[0-9]+\", channels=1, classes=3) # Pass Data IO Interface to Data IO class from data_loading.data_io import Data_IO data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data/\" data_io = Data_IO(interface, data_path)","title":"Usage of MIScnn Data IO Interfaces"},{"location":"IO-Interfaces/#available-data-io-interfaces-provided-by-miscnn","text":"","title":"Available Data IO Interfaces provided by MIScnn"},{"location":"IO-Interfaces/#nifti-io-interface","text":"Data I/O Interface for NIfTI files. The Neuroimaging Informatics Technology Initiative file format is designed to contain brain images from e.g. magnetic resonance tomography. Nevertheless, it is currently broadly used for any 3D medical image data. # Import NIfTI IO Interface and initialize it from data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_00[0-9]+\", channels=1, classes=3) # Pass Interface to Data IO class data_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data.interpolated/\" data_io = Data_IO(interface, data_path)","title":"NIfTI IO Interface"},{"location":"IO-Interfaces/#image-io-interface","text":"Coming soon for PNG, JPEG and TIF :)","title":"Image IO Interface"},{"location":"IO-Interfaces/#dicom-io-interface","text":"Coming soon for DICOM data :)","title":"DICOM IO Interface"},{"location":"IO-Interfaces/#dictionary-io-interface","text":"Data I/O Interface for python dictionaries. This interface uses the basic-python dictionary to load and save data. Therefore the complete data management happens in the memory. Therefore, for common data set sizes this is NOT recommended! It is advised to use already provided I/O interfaces of this package or to implement a custom I/O interface for perfect usability. Dictionary structure: Key: sample_index Value: Tuple containing: (0) image as numpy array (1) optional segmentation as numpy array (2) optional prediction as numpy array (3) optional details Example: # Create dictionary containing the data set dict = {\"case_00001\": (image1, segmentation1), \"case_00002\": (image2, segmentation2)} # Import Data IO Interface and pass dictionary to it from data_loading.interfaces import Dictionary_interface interface = Dictionary_interface(dict, channels=1, classes=2, three_dim=True) # Pass Interface to Data IO class data_path = \"/home/mudomini/MIScnn_tmp/\" data_io = Data_IO(interface, data_path)","title":"Dictionary IO Interface"},{"location":"IO-Interfaces/#creation-of-custom-data-io-interfaces","text":"todo todo","title":"Creation of custom Data IO Interfaces"},{"location":"IO-Interfaces/#abstract-base-class-for-data-io-interfaces","text":"MIScnn also offers a documented Abstract Base Class for simple creation of custom Data IO Interfaces for your specific needs. A Data IO Interface inherits the abstract_io class with the following methods: initialize, load_image, load_segmentation, load_prediction, save_prediction, load_details. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for the Data IO class # #-----------------------------------------------------# \"\"\" An abstract base class for a Data_IO interface. Methods: __init__ Object creation function initialize: Prepare the data set and create indices list load_image: Load an image load_segmentation: Load a segmentation load_prediction: Load a prediction from file load_details: Load optional information save_prediction: Save a prediction to file \"\"\" class Abstract_IO(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the I/O interface object creation. This function can be used to pass variables in the custom I/O interface. The only required passed variable is the number of channels in the images, the number of classes in the segmentation and the dimension of the data. Parameter: channels (integer): Number of channels of the image (grayscale:1, RGB:3) classes (integer): Number of classes in the segmentation (binary:2, multi-class:3+) three_dim (boolean): Variable to express, if the data is two or three dimensional Return: None \"\"\" @abstractmethod def __init__(self, channels=1, classes=2, three_dim=False): self.channels = channels self.classes = classes self.three_dim = three_dim pass #---------------------------------------------# # initialize # #---------------------------------------------# \"\"\" Initialize and prepare the image data set, return the number of samples in the data set Parameter: input_path (string): Path to the input data directory, in which all imaging data have to be accessible Return: indices_list [list]: List of indices. The Data_IO class will iterate over this list and call the load_image and load_segmentation functions providing the current index. This can be used to train/predict on just a subset of the data set. e.g. indices_list = [0,1,9] -> load_image(0) | load_image(1) | load_image(9) \"\"\" @abstractmethod def initialize(self, input_path): pass #---------------------------------------------# # load_image # #---------------------------------------------# \"\"\" Load the image with the index i from the data set and return it as a numpy matrix. Be aware that MIScnn only supports a last_channel structure. 2D: (x,y,channel) or (x,y) 3D: (x,y,z,channel) or (x,y,z) Parameter: index (variable): An index from the provided indices_list of the initialize function Return: image [numpy matrix]: A numpy matrix/array containing the image \"\"\" @abstractmethod def load_image(self, i): pass #---------------------------------------------# # load_segmentation # #---------------------------------------------# \"\"\" Load the segmentation of the image with the index i from the data set and return it as a numpy matrix. Be aware that MIScnn only supports a last_channel structure. 2D: (x,y,channel) or (x,y) 3D: (x,y,z,channel) or (x,y,z) Parameter: index (variable): An index from the provided indices_list of the initialize function Return: seg [numpy matrix]: A numpy matrix/array containing the segmentation \"\"\" @abstractmethod def load_segmentation(self, i): pass #---------------------------------------------# # load_prediction # #---------------------------------------------# \"\"\" Load the prediction of the image with the index i from the output directory and return it as a numpy matrix. Parameter: index (variable): An index from the provided indices_list of the initialize function output_path (string): Path to the output directory in which MIScnn predictions are stored. Return: pred [numpy matrix]: A numpy matrix/array containing the prediction \"\"\" @abstractmethod def load_prediction(self, i, output_path): pass #---------------------------------------------# # load_details # #---------------------------------------------# \"\"\" Load optional details during sample creation. This function can be used to parse whatever information you want into the sample object. This enables usage of these information in custom preprocessing subfunctions. Example: Slice thickness / voxel spacing Parameter: index (variable): An index from the provided indices_list of the initialize function Return: dict [dictionary]: A basic Python dictionary \"\"\" @abstractmethod def load_details(self, i): pass #---------------------------------------------# # save_prediction # #---------------------------------------------# \"\"\" Backup the prediction of the image with the index i into the output directory. Parameter: pred (numpy matrix): MIScnn computed prediction for the sample index index (variable): An index from the provided indices_list of the initialize function output_path (string): Path to the output directory in which MIScnn predictions are stored. This directory will be created if not existent Return: None \"\"\" @abstractmethod def save_prediction(self, pred, i, output_path): pass","title":"Abstract Base Class for Data IO Interfaces"},{"location":"Install/","text":"Prerequisites Commonly, you do not have to care about prerequisites because the package manager pip automatically install anything you need. BUT: MIScnn does NOT use Tensorflow 2.0 (right now)! Therefore, if you have a higher Tensorflow or Keras version, please install the two modules manually with the correct version or in a separate package environment (e.g. with Conda ). MIScnn currently only supports Tensorflow 1.13 and Keras 2.2.4. Keras install instructions: https://keras.io/#installation MIScnn Installation There are two ways to install MIScnn: Install MIScnn from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows or in a virtual environment without root, you will need to remove sudo to run the commands below. sudo pip install miscnn Alternatively: install MIScnn from the GitHub source: First, clone MIScnn using git: git clone https://github.com/frankkramer-lab/MIScnn Then, cd to the MIScnn folder and run the install command: cd MIScnn sudo python setup.py install","title":"Install"},{"location":"Install/#prerequisites","text":"Commonly, you do not have to care about prerequisites because the package manager pip automatically install anything you need. BUT: MIScnn does NOT use Tensorflow 2.0 (right now)! Therefore, if you have a higher Tensorflow or Keras version, please install the two modules manually with the correct version or in a separate package environment (e.g. with Conda ). MIScnn currently only supports Tensorflow 1.13 and Keras 2.2.4. Keras install instructions: https://keras.io/#installation","title":"Prerequisites"},{"location":"Install/#miscnn-installation","text":"There are two ways to install MIScnn: Install MIScnn from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows or in a virtual environment without root, you will need to remove sudo to run the commands below. sudo pip install miscnn Alternatively: install MIScnn from the GitHub source: First, clone MIScnn using git: git clone https://github.com/frankkramer-lab/MIScnn Then, cd to the MIScnn folder and run the install command: cd MIScnn sudo python setup.py install","title":"MIScnn Installation"},{"location":"Metrics/","text":"A loss function or metric (or objective function, or optimization score function) is a scoring function to evaluate the current predictive power and performance of a model. A loss function is required for training and have to be decreasing, if the model performance is increasing. During the training process, the neural network model varies weights of its neurons and tries to optimize the loss function to be as small as possible. A model can only use one loss function. Metrics are for user evaluation in order to give insights on the training process or the current performance status of the model. It is possible to pass multiple metrics to the model. MIScnn loss functions and metrics are based on Keras loss/metric functions. Any loss or metric function defined in Keras, in miscnn.neural_network.metrics or any custom function, which follows the Keras metric guidelines, can be used in MIScnn. Usage of loss functions and metrics The loss function and metrics can be defined at the neural network model class initialization. It is possible to pass the desired loss function with the 'loss' tag and the metrics within a standard python list with the 'metrics' tag. # Import desired loss functions or metrics from miscnn.neural_network.metrics import tversky_loss, dice_soft import keras # Add the loss function & metrics to the neural network model model = Neural_Network(preprocessor=pp, loss=tversky_loss, metrics=[dice_soft, keras.losses.categorical_crossentropy]) If no loss or metrics are provided, MIScnn uses the Tversky loss function and no metrics as default. Available loss functions and metrics provided by MIScnn Tversky loss A generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convo- lutional deep neural networks. Reference: https://arxiv.org/abs/1706.05721 from miscnn.neural_network.metrics import tversky_loss model = Neural_Network(preprocessor=pp, loss=tversky_loss) Sum of Tversky loss and Cross-entropy Based on the idea of Fabian Isensee et al. for using the sum of the Dice Similarity Coefficient and the Cross-Entropy, MIScnn implemented a loss function based on the sum of the Tversky loss & Cross-Entropy loss function. Reference Fabian Isensee et al: http://arxiv.org/abs/1809.10486 from miscnn.neural_network.metrics import tversky_crossentropy model = Neural_Network(preprocessor=pp, loss=tversky_crossentropy) Dice Similarity Coefficient (DSC) The Dice Similarity Coefficient is one the most popular loss functions for training deep learning models in medical image segmentation. It has the ability the handle strong class imbalance which is common in medical imaging. The standard Dice Similarity Coefficient is suited for binary medical image segmentation problems. Reference: -> add wikipedia link from miscnn.neural_network.metrics import dice_coefficient_loss model = Neural_Network(preprocessor=pp, loss=dice_coefficient_loss) Soft Dice Similarity Coefficient The Soft Dice Similarity Coefficient is an extension of the standard Dice Similarity Coeffcient. The only difference is, that the soft DSC is normalized on the number of classes. Therefore, the soft Dice Similarity Coefficient is more suited for multi-class (>2 classes) medical image segmentation problems. Reference: -> add wikipedia link from miscnn.neural_network.metrics import dice_soft_loss model = Neural_Network(preprocessor=pp, loss=dice_soft_loss) Sum of Soft Dice Similarity Coefficient and Cross-entropy In diverse medical image segmentation challenges, the winner Fabian Isensee et al. is always using the sum of the soft Dice Similarity Coeffcient and the Cross-Entropy in order to achieve the best performance. Additionally, Fabian Isensee et al. also uses this loss function in his powerful software nnUnet. Reference: http://arxiv.org/abs/1809.10486 from miscnn.neural_network.metrics import dice_crossentropy model = Neural_Network(preprocessor=pp, loss=dice_crossentropy)","title":"Metrics"},{"location":"Metrics/#usage-of-loss-functions-and-metrics","text":"The loss function and metrics can be defined at the neural network model class initialization. It is possible to pass the desired loss function with the 'loss' tag and the metrics within a standard python list with the 'metrics' tag. # Import desired loss functions or metrics from miscnn.neural_network.metrics import tversky_loss, dice_soft import keras # Add the loss function & metrics to the neural network model model = Neural_Network(preprocessor=pp, loss=tversky_loss, metrics=[dice_soft, keras.losses.categorical_crossentropy]) If no loss or metrics are provided, MIScnn uses the Tversky loss function and no metrics as default.","title":"Usage of loss functions and metrics"},{"location":"Metrics/#available-loss-functions-and-metrics-provided-by-miscnn","text":"","title":"Available loss functions and metrics provided by MIScnn"},{"location":"Metrics/#tversky-loss","text":"A generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convo- lutional deep neural networks. Reference: https://arxiv.org/abs/1706.05721 from miscnn.neural_network.metrics import tversky_loss model = Neural_Network(preprocessor=pp, loss=tversky_loss)","title":"Tversky loss"},{"location":"Metrics/#sum-of-tversky-loss-and-cross-entropy","text":"Based on the idea of Fabian Isensee et al. for using the sum of the Dice Similarity Coefficient and the Cross-Entropy, MIScnn implemented a loss function based on the sum of the Tversky loss & Cross-Entropy loss function. Reference Fabian Isensee et al: http://arxiv.org/abs/1809.10486 from miscnn.neural_network.metrics import tversky_crossentropy model = Neural_Network(preprocessor=pp, loss=tversky_crossentropy)","title":"Sum of Tversky loss and Cross-entropy"},{"location":"Metrics/#dice-similarity-coefficient-dsc","text":"The Dice Similarity Coefficient is one the most popular loss functions for training deep learning models in medical image segmentation. It has the ability the handle strong class imbalance which is common in medical imaging. The standard Dice Similarity Coefficient is suited for binary medical image segmentation problems. Reference: -> add wikipedia link from miscnn.neural_network.metrics import dice_coefficient_loss model = Neural_Network(preprocessor=pp, loss=dice_coefficient_loss)","title":"Dice Similarity Coefficient (DSC)"},{"location":"Metrics/#soft-dice-similarity-coefficient","text":"The Soft Dice Similarity Coefficient is an extension of the standard Dice Similarity Coeffcient. The only difference is, that the soft DSC is normalized on the number of classes. Therefore, the soft Dice Similarity Coefficient is more suited for multi-class (>2 classes) medical image segmentation problems. Reference: -> add wikipedia link from miscnn.neural_network.metrics import dice_soft_loss model = Neural_Network(preprocessor=pp, loss=dice_soft_loss)","title":"Soft Dice Similarity Coefficient"},{"location":"Metrics/#sum-of-soft-dice-similarity-coefficient-and-cross-entropy","text":"In diverse medical image segmentation challenges, the winner Fabian Isensee et al. is always using the sum of the soft Dice Similarity Coeffcient and the Cross-Entropy in order to achieve the best performance. Additionally, Fabian Isensee et al. also uses this loss function in his powerful software nnUnet. Reference: http://arxiv.org/abs/1809.10486 from miscnn.neural_network.metrics import dice_crossentropy model = Neural_Network(preprocessor=pp, loss=dice_crossentropy)","title":"Sum of Soft Dice Similarity Coefficient and Cross-entropy"},{"location":"Model/","text":"This class provides functionality for handling all model methods. This class runs the whole pipeline and uses a Preprocessor instance to obtain batches. With an initialized Neural Network model instance, it is possible to run training, prediction and evaluations. Methods Initialization Neural_Network(preprocessor, architecture=Architecture(), loss=tversky_loss, metrics=[dice_soft], learninig_rate=0.0001, batch_queue_size=2, workers=1, gpu_number=1) Initialization function for creating a Neural Network (model) object. Arguments: - preprocessor: Preprocessor class instance which provides the Neural Network with batches. - architecture: Instance of a neural network model Architecture class instance. By default, a standard U-Net is used as Architecture. - loss: The Metric function which is used as loss for training. - metrics: List of one or multiple Metric Functions, which will be shown during training. - learninig_rate: Learning rate in which weights of the neural network will be updated. - batch_queue_size: The batch queue size is the number of previously prepared batches in the cache during runtime. - workers: Number of workers/threads which preprocess batches during runtime. - gpu_number: Number of GPUs, which will be used for training. Returns: A Neural Network Model class object. Example: model = miscnn.Neural_Network(preprocessor=pp) model.train(sample_list[0:100], epochs=50) train train(sample_list, epochs=20, iterations=None, callbacks=[]) Fitting function for the Neural Network model using the provided list of sample indices. Arguments: - sample_list: A list of sample indicies which will be used for training. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Keras Callback classes for custom evaluation. Returns: None. Example: model.train(sample_list[0:100], epochs=50) predict predict(sample_list, direct_output=False) Prediction function for the Neural Network model. The fitted model will predict a segmentation for the provided list of sample indices. Arguments: - sample_list: A list of sample indicies for which a segmentation prediction will be computed. - direct_output: Boolean, whether computed predictions will be output as the return of this function or if the predictions will be saved with the save_prediction method defined in the provided Data I/O interface. Returns: None or a list of NumPy arrays containing predictions. Example: # Passing output of predictions into Data I/O interface - (intended for disk storage) model.predict(sample_list[100:120]) # Direct output of predictions into variable (memory) predictions = model.predict(sample_list[100:120], direct_output=True) evaluate evaluate(training_samples, validation_samples, epochs=20, iterations=None, callbacks=[]): Evaluation function for the Neural Network model using the provided lists of sample indices for training and validation. It is also possible to pass custom Callback classes in order to obtain more information. Arguments: - training_samples: A list of sample indicies which will be used for training. - validation_samples: A list of sample indicies which will be used for validation. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Keras Callback classes for custom evaluation. Returns: Keras history object (gathered fitting information and evaluation results of the validation). Example: history = model.evaluate(training_samples=sample_list[0:80], validation_samples=sample_list[80:100], iterations=10) reset_weights reset_weights() Re-initialize weights of the neural network model. Arguments: None. Returns: None. Example: # Fit weights of model model.train(sample_list[0:100], epochs=50) # Throw fitted weights away to default weights model.reset_weights() dump dump(file_path) Dump neural network model and its weights to file. Arguments: - file_path: Output path, at which the model will be stored. Returns: None. Example: # Fit weights of model model.train(sample_list[0:100], epochs=50) # Save fitted model for reusability model.dump(\"my_model.hdf5\") load load(file_path, custom_objects={}): Load neural network model and its weights from a file. After loading, the model will be compiled. Arguments: - file_path: Input path, from which the model will be loaded. - custom_objects: Dictionary for custom objects for compiling (e.g. custom-defined metrics). Returns: None. Example: # Save fitted model for reusability model.load(\"my_model.hdf5\") # Predict segmentation of some samples model.predict(sample_list[100:120])","title":"Neural Network Model"},{"location":"Model/#methods","text":"","title":"Methods"},{"location":"Model/#initialization","text":"Neural_Network(preprocessor, architecture=Architecture(), loss=tversky_loss, metrics=[dice_soft], learninig_rate=0.0001, batch_queue_size=2, workers=1, gpu_number=1) Initialization function for creating a Neural Network (model) object. Arguments: - preprocessor: Preprocessor class instance which provides the Neural Network with batches. - architecture: Instance of a neural network model Architecture class instance. By default, a standard U-Net is used as Architecture. - loss: The Metric function which is used as loss for training. - metrics: List of one or multiple Metric Functions, which will be shown during training. - learninig_rate: Learning rate in which weights of the neural network will be updated. - batch_queue_size: The batch queue size is the number of previously prepared batches in the cache during runtime. - workers: Number of workers/threads which preprocess batches during runtime. - gpu_number: Number of GPUs, which will be used for training. Returns: A Neural Network Model class object. Example: model = miscnn.Neural_Network(preprocessor=pp) model.train(sample_list[0:100], epochs=50)","title":"Initialization"},{"location":"Model/#train","text":"train(sample_list, epochs=20, iterations=None, callbacks=[]) Fitting function for the Neural Network model using the provided list of sample indices. Arguments: - sample_list: A list of sample indicies which will be used for training. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Keras Callback classes for custom evaluation. Returns: None. Example: model.train(sample_list[0:100], epochs=50)","title":"train"},{"location":"Model/#predict","text":"predict(sample_list, direct_output=False) Prediction function for the Neural Network model. The fitted model will predict a segmentation for the provided list of sample indices. Arguments: - sample_list: A list of sample indicies for which a segmentation prediction will be computed. - direct_output: Boolean, whether computed predictions will be output as the return of this function or if the predictions will be saved with the save_prediction method defined in the provided Data I/O interface. Returns: None or a list of NumPy arrays containing predictions. Example: # Passing output of predictions into Data I/O interface - (intended for disk storage) model.predict(sample_list[100:120]) # Direct output of predictions into variable (memory) predictions = model.predict(sample_list[100:120], direct_output=True)","title":"predict"},{"location":"Model/#evaluate","text":"evaluate(training_samples, validation_samples, epochs=20, iterations=None, callbacks=[]): Evaluation function for the Neural Network model using the provided lists of sample indices for training and validation. It is also possible to pass custom Callback classes in order to obtain more information. Arguments: - training_samples: A list of sample indicies which will be used for training. - validation_samples: A list of sample indicies which will be used for validation. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Keras Callback classes for custom evaluation. Returns: Keras history object (gathered fitting information and evaluation results of the validation). Example: history = model.evaluate(training_samples=sample_list[0:80], validation_samples=sample_list[80:100], iterations=10)","title":"evaluate"},{"location":"Model/#reset_weights","text":"reset_weights() Re-initialize weights of the neural network model. Arguments: None. Returns: None. Example: # Fit weights of model model.train(sample_list[0:100], epochs=50) # Throw fitted weights away to default weights model.reset_weights()","title":"reset_weights"},{"location":"Model/#dump","text":"dump(file_path) Dump neural network model and its weights to file. Arguments: - file_path: Output path, at which the model will be stored. Returns: None. Example: # Fit weights of model model.train(sample_list[0:100], epochs=50) # Save fitted model for reusability model.dump(\"my_model.hdf5\")","title":"dump"},{"location":"Model/#load","text":"load(file_path, custom_objects={}): Load neural network model and its weights from a file. After loading, the model will be compiled. Arguments: - file_path: Input path, from which the model will be loaded. - custom_objects: Dictionary for custom objects for compiling (e.g. custom-defined metrics). Returns: None. Example: # Save fitted model for reusability model.load(\"my_model.hdf5\") # Predict segmentation of some samples model.predict(sample_list[100:120])","title":"load"},{"location":"Preprocessor/","text":"This class provides functionality for handling all preprocessing methods. This includes diverse optional processing subfunctions like resampling, clipping, normalization or custom subfcuntions. This class processes the data into batches which are ready to be used for training, prediction and validation. The user is only required to create an instance of the Preprocessor class with the desired specifications and Data IO instance (optional also Data Augmentation instance). Methods Initialization Preprocessor(data_io, batch_size, subfunctions=[], data_aug=Data_Augmentation(), prepare_subfunctions=False, prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=None) Initialization function for creating a Preprocessor object. Arguments: - data_io: Data IO class instance which handles all I/O operations. - batch_size: Number of samples inside a single batch. - subfunctions: List of Subfunctions classes which will be SEQUENTIALLY executed on the data set. - data_aug: Data Augmentation class instance which performs diverse data augmentation techniques. If no Data Augmentation is provided, an instance with default settings will be created. Use data_aug=None, if you want no data augmentation at all. - prepare_subfunctions: Boolean, whether all subfunctions should be prepared and backup to disk before starting the batch generation (True), or should the subfunctions preprocessing be performed during runtime? (False). - prepare_batches: Boolean, whether all batches should be prepared and backup to disk before starting the training (True), or should the batches be created during runtime? (False). - analysis: String. Modus selection of analysis type. Options: [\"fullimage\", \"patchwise-crop\", \"patchwise-grid\"] - patch_shape: Integer tuple. Size and shape of a patch. Returns: A Preprocessor class object. Have to be passed to the Neural Network Model class. Additional Information: Modus selection of analysis type. The analysis type will define how patches are created. - \"fullimage\": Analysis of complete image data - \"patchwise-crop\": Analysis of random cropped patches from the image - \"patchwise-grid\": Analysis of patches by splitting the image into a grid Example: from processing.preprocessor import Preprocessor from processing.subfunctions import Clipping, Normalization, Resampling sf = [Clipping(min=-100, max=500), Normalization(z_score=True), Resampling((3.22, 1.62, 1.62))] pp = Preprocessor(data_io, data_aug=None, batch_size=2, subfunctions=sf, prepare_subfunctions=True, prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=(80,160,160))","title":"Preprocessor"},{"location":"Preprocessor/#methods","text":"","title":"Methods"},{"location":"Preprocessor/#initialization","text":"Preprocessor(data_io, batch_size, subfunctions=[], data_aug=Data_Augmentation(), prepare_subfunctions=False, prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=None) Initialization function for creating a Preprocessor object. Arguments: - data_io: Data IO class instance which handles all I/O operations. - batch_size: Number of samples inside a single batch. - subfunctions: List of Subfunctions classes which will be SEQUENTIALLY executed on the data set. - data_aug: Data Augmentation class instance which performs diverse data augmentation techniques. If no Data Augmentation is provided, an instance with default settings will be created. Use data_aug=None, if you want no data augmentation at all. - prepare_subfunctions: Boolean, whether all subfunctions should be prepared and backup to disk before starting the batch generation (True), or should the subfunctions preprocessing be performed during runtime? (False). - prepare_batches: Boolean, whether all batches should be prepared and backup to disk before starting the training (True), or should the batches be created during runtime? (False). - analysis: String. Modus selection of analysis type. Options: [\"fullimage\", \"patchwise-crop\", \"patchwise-grid\"] - patch_shape: Integer tuple. Size and shape of a patch. Returns: A Preprocessor class object. Have to be passed to the Neural Network Model class. Additional Information: Modus selection of analysis type. The analysis type will define how patches are created. - \"fullimage\": Analysis of complete image data - \"patchwise-crop\": Analysis of random cropped patches from the image - \"patchwise-grid\": Analysis of patches by splitting the image into a grid Example: from processing.preprocessor import Preprocessor from processing.subfunctions import Clipping, Normalization, Resampling sf = [Clipping(min=-100, max=500), Normalization(z_score=True), Resampling((3.22, 1.62, 1.62))] pp = Preprocessor(data_io, data_aug=None, batch_size=2, subfunctions=sf, prepare_subfunctions=True, prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=(80,160,160))","title":"Initialization"},{"location":"Sample/","text":"The Sample class is provides objects to store all kinds of information for a single sample. It mandatory variables of a Samples are its index (id), an image and some information of the image like its shape and the number of channels. The Sample class can also store a optional segmentation with its number of classes, as well as a predicted segmentation from the model. It is also possible to add additional custom information in the details dictionary. This feature can be exploited in later custom interfaces like in a Subfunction. The Data IO class will automatically create Sample objects during a Pipeline run. It is also possible to obtain all created Sample objects by the following call: sample_list = data_io.get_indiceslist() Methods Initialization Sample(index, image, channels, classes) Initialization function for creating a Sample object. Arguments: - index: Index (String) of a sample. - image: NumPy array containing the image. - channels: Number of channels of the image (dimension of last layer). - classes: Number of classes of the segmentation. Returns: A Sample class object. Example: sample = data_io.sample_loader(index=\"case_00001\", load_seg=True) img = sample.img_data","title":"Sample"},{"location":"Sample/#methods","text":"","title":"Methods"},{"location":"Sample/#initialization","text":"Sample(index, image, channels, classes) Initialization function for creating a Sample object. Arguments: - index: Index (String) of a sample. - image: NumPy array containing the image. - channels: Number of channels of the image (dimension of last layer). - classes: Number of classes of the segmentation. Returns: A Sample class object. Example: sample = data_io.sample_loader(index=\"case_00001\", load_seg=True) img = sample.img_data","title":"Initialization"},{"location":"Subfunctions/","text":"Image preprocessing is defined as a method or technique which modify the image before passing it to the neural network model. The aim of preprocessing methods is to extensively increase performance due to simplification of information. In medical image segmentation, it is required to perform extensive preprocessing to the medical images. Common preprocessing methods range from intensity value normalization to image resizing. The preprocessing is not only required for performance increase, but also to reduce the image information content in order to be fit-able in the neural network model in terms of GPU RAM size. In the current state-of-the-art mdeical image segmentation pipelines, several preprocessing methods are common: Resampling slice thickness, resizing images to fit into GPUs and intensity value normalization. In order to provide a wide variety of preprocessing methods, MIScnn offers the Subfunction modularity. The user is able to create a list of desired preprocessing functions (in MIScnn called Subfunctions) and pass them to the Preprocessor class, which allows high configurability for all scenarios. Usage of MIScnn Subfunctions A list of Subfunctions can be passed to the Preprocessor class initialization. The Preprocessor automatically uses the list of Subfunctions and sequentially runs the Subfunctions on the data set. For predictions, the medical images will also be automatically postprocessed in order to restore the predicted segmentation for the original image features (e.g. resizing back to original size). # Import desired Subfunctions from processing.subfunctions import Normalization from processing.subfunctions import Resampling # Initialize Subfunctions into a list sf_normalization = Normalization(z_score=True) sf_resampling = Resampling(new_spacing=(3.22, 1.62, 1.62)) sf = [sf_normalization, sf_resampling] # Pass list of Subfunctions to Preprocessor class pp = Preprocessor(data_io, batch_size=2, subfunctions=sf) Available Subfunctions provided by MIScnn Intensity Value Normalization A Normalization Subfunction class which normalizes the intensity pixel values of an image using the Z-Score technique (default setting) or through scaling to [0,1]. from processing.subfunctions import Normalization sf_norm = Normalization(z_score=True) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_norm]) Resizing A Resize Subfunction class which resizes an images according to a desired shape. from processing.subfunctions import Resize sf_resize = Resize(new_shape=(128,128,128)) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_resize]) Resampling A Resampling Subfunction class which resizes an images according to a desired voxel spacing. This function only works with already cached \"spacing\" matrix in the detailed information dictionary of the sample. from processing.subfunctions import Resampling sf_resampling = Resampling(new_spacing=(3.22, 1.62, 1.62)) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_resampling]) Clipping A Clipping Subfunction class which can be used for clipping intensity pixel values on a certain range. from processing.subfunctions import Clipping sf_clip = Clipping(min=25, max=75) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_clip]) Creation of custom Subfunctions todo todo Abstract Base Class for Subfunctions MIScnn also offers a documented Abstract Base Class for easier creating of custom Subfunctions for your specific needs. A Subfunction is a class which consists of an init, preprocessing and postprocessing function. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for the Subfunction class # #-----------------------------------------------------# \"\"\" An abstract base class for a processing Subfcuntion class. Methods: __init__ Object creation function preprocessing: Transform the imaging data postprocessing: Transform the predicted segmentation \"\"\" class Abstract_Subfunction(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the Subfunction object creation. This function can be used to pass variables and options in the Subfunction instance. The are no mandatory required parameters for the initialization. Parameter: None Return: None \"\"\" @abstractmethod def __init__(self): pass #---------------------------------------------# # preprocessing # #---------------------------------------------# \"\"\" Transform the image according to the subfunction during preprocessing (training + prediction). This is an in-place transformation of the sample object, therefore nothing is returned. It is possible to pass configurations through the initialization function of this class. Parameter: sample (Sample class): Sample class object containing the imaging data (sample.img_data) and optional segmentation data (sample.seg_data) training (boolean): Boolean variable indicating, if segmentation data is present at the sample object. If training is true, segmentation data in the sample object is available, if training is false, sample.seg_data is None Return: None \"\"\" @abstractmethod def preprocessing(self, sample, training=True): pass #---------------------------------------------# # postprocessing # #---------------------------------------------# \"\"\" Transform the prediction according to the subfunction during postprocessing (prediction). This is NOT an in-place transformation of the prediction, therefore it is REQUIRED to return the processed prediction array. It is possible to pass configurations through the initialization function of this class. Parameter: prediction (numpy array): Numpy array of the predicted segmentation Return: prediction (numpy array): Numpy array of processed predicted segmentation \"\"\" @abstractmethod def postprocessing(self, prediction): return prediction","title":"Subfunctions"},{"location":"Subfunctions/#usage-of-miscnn-subfunctions","text":"A list of Subfunctions can be passed to the Preprocessor class initialization. The Preprocessor automatically uses the list of Subfunctions and sequentially runs the Subfunctions on the data set. For predictions, the medical images will also be automatically postprocessed in order to restore the predicted segmentation for the original image features (e.g. resizing back to original size). # Import desired Subfunctions from processing.subfunctions import Normalization from processing.subfunctions import Resampling # Initialize Subfunctions into a list sf_normalization = Normalization(z_score=True) sf_resampling = Resampling(new_spacing=(3.22, 1.62, 1.62)) sf = [sf_normalization, sf_resampling] # Pass list of Subfunctions to Preprocessor class pp = Preprocessor(data_io, batch_size=2, subfunctions=sf)","title":"Usage of MIScnn Subfunctions"},{"location":"Subfunctions/#available-subfunctions-provided-by-miscnn","text":"","title":"Available Subfunctions provided by MIScnn"},{"location":"Subfunctions/#intensity-value-normalization","text":"A Normalization Subfunction class which normalizes the intensity pixel values of an image using the Z-Score technique (default setting) or through scaling to [0,1]. from processing.subfunctions import Normalization sf_norm = Normalization(z_score=True) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_norm])","title":"Intensity Value Normalization"},{"location":"Subfunctions/#resizing","text":"A Resize Subfunction class which resizes an images according to a desired shape. from processing.subfunctions import Resize sf_resize = Resize(new_shape=(128,128,128)) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_resize])","title":"Resizing"},{"location":"Subfunctions/#resampling","text":"A Resampling Subfunction class which resizes an images according to a desired voxel spacing. This function only works with already cached \"spacing\" matrix in the detailed information dictionary of the sample. from processing.subfunctions import Resampling sf_resampling = Resampling(new_spacing=(3.22, 1.62, 1.62)) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_resampling])","title":"Resampling"},{"location":"Subfunctions/#clipping","text":"A Clipping Subfunction class which can be used for clipping intensity pixel values on a certain range. from processing.subfunctions import Clipping sf_clip = Clipping(min=25, max=75) pp = Preprocessor(data_io, batch_size=2, subfunctions=[sf_clip])","title":"Clipping"},{"location":"Subfunctions/#creation-of-custom-subfunctions","text":"todo todo","title":"Creation of custom Subfunctions"},{"location":"Subfunctions/#abstract-base-class-for-subfunctions","text":"MIScnn also offers a documented Abstract Base Class for easier creating of custom Subfunctions for your specific needs. A Subfunction is a class which consists of an init, preprocessing and postprocessing function. #-----------------------------------------------------# # Library imports # #-----------------------------------------------------# # External libraries from abc import ABC, abstractmethod #-----------------------------------------------------# # Abstract Interface for the Subfunction class # #-----------------------------------------------------# \"\"\" An abstract base class for a processing Subfcuntion class. Methods: __init__ Object creation function preprocessing: Transform the imaging data postprocessing: Transform the predicted segmentation \"\"\" class Abstract_Subfunction(ABC): #---------------------------------------------# # __init__ # #---------------------------------------------# \"\"\" Functions which will be called during the Subfunction object creation. This function can be used to pass variables and options in the Subfunction instance. The are no mandatory required parameters for the initialization. Parameter: None Return: None \"\"\" @abstractmethod def __init__(self): pass #---------------------------------------------# # preprocessing # #---------------------------------------------# \"\"\" Transform the image according to the subfunction during preprocessing (training + prediction). This is an in-place transformation of the sample object, therefore nothing is returned. It is possible to pass configurations through the initialization function of this class. Parameter: sample (Sample class): Sample class object containing the imaging data (sample.img_data) and optional segmentation data (sample.seg_data) training (boolean): Boolean variable indicating, if segmentation data is present at the sample object. If training is true, segmentation data in the sample object is available, if training is false, sample.seg_data is None Return: None \"\"\" @abstractmethod def preprocessing(self, sample, training=True): pass #---------------------------------------------# # postprocessing # #---------------------------------------------# \"\"\" Transform the prediction according to the subfunction during postprocessing (prediction). This is NOT an in-place transformation of the prediction, therefore it is REQUIRED to return the processed prediction array. It is possible to pass configurations through the initialization function of this class. Parameter: prediction (numpy array): Numpy array of the predicted segmentation Return: prediction (numpy array): Numpy array of processed predicted segmentation \"\"\" @abstractmethod def postprocessing(self, prediction): return prediction","title":"Abstract Base Class for Subfunctions"},{"location":"Tutorials/","text":"Coming soon!","title":"Tutorials"},{"location":"Tutorials/#coming-soon","text":"","title":"Coming soon!"},{"location":"Usage/","text":"Core steps of the MIScnn pipeline The MIScnn pipeline is represented by 4 core classes: Data I/O Data Augmentation (optional) Preprocessor Neural Network Model These classes handle all required steps for medical image segmentation and can be extensively customized. All classes, except the Data Augmentation class, use switchable interfaces which results into high configurability and offers simple integration of user-defined solutions. Workflow of MIScnn Import the MIScnn Module Before constructing our Medical Image Segmentation pipeline, we are going to import the MIScnn modules which will provide our core classes and functions. import miscnn 1) Data I/O The first step in the MIScnn pipeline is to establish the Data I/O class. The Data I/O handles the complete data loading of biomedical images, its segmentation and storage of predicted files, as well as temporary files like batches. In order to open the MIScnn pipeline for a various data formats and structures, the data I/O class uses switchable I/O interfaces. It is possible to use already provided I/O interfaces with a fixed data structure or custom I/O interfaces for integration of your specific data structure into the MIScnn pipeline. Before initializing an instance of the Data I/O class, we have to select an I/O interface. This interface have to load our data set into the MIScnn pipeline. In our example, we are using the KiTS19 data set which are kidney tumor CT scans encoded in the NIfTI format. Therefore, we call the provided NIfTI interface with a single channel (grayscale image) and 3 classes for the segmentation (background, kidney and tumor). from miscnn.data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) Afterwards, we can initialize the Data I/O class by passing our newly created interface and the path to the data set to the constructor. data_path = \"/home/mudomini/projects/kits19/data/\" data_io = miscnn.Data_IO(interface, data_path) 2) Preprocessor Finally, we can initialize the Preprocessor class. Therefore, we are passing our Data I/O Interface to the Preprocessor initialization. Additionally, we configure our Preprocessor to create batches on-the-fly (preprare_batches=False) by random cropping patches with size 80x160x160 out of the image. Also a batch should contain 2 patches. # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160)) Specefic Data Augmentation If you want to specify the Data Augmentation, it is possible to configure the Data Augmentation class. A Preprocessor object with default parameters automatically initialize a Data Augmentation class with default values, but here we initialize it by hand to illustrate the exact workflow of the MIScnn pipeline. The parameters for the Data Augmentation configure which augmentation techniques should be applied to the data set. In this case, we are using all possible augmentation techniques in order to run extensive data augmentation and avoid overfitting # Create and configure the Data Augmentation class data_aug = miscnn.Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True, brightness=True, contrast=True, gamma=True, gaussian_noise=True) # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, data_aug=data_aug, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160)) 3) Neural Network Model Now it's time to initialize our final object: The Neural Network. With the Neural Network class, we are able to run training and prediction operations. But before, we have to decide which Neural Network Architecture and Metric we want to use. In order to show the simplicity and performance of MIScnn, we stick with the simple 3D U-Net Architecture for our Neural Network without any special tricks or optimizations. For training, we are using the Soft Dice as loss. # Import standard U-Net architecture and Soft Dice from miscnn.neural_network.architecture.unet.standard import Architecture from miscnn.neural_network.metrics import dice_soft unet_standard = Architecture() # Create a deep learning neural network model with a standard U-Net architecture model = miscnn.Neural_Network(preprocessor=pp, architecture=unet_standard, loss=dice_soft) Now, let's run a model training on our data set for 500 epochs and, afterwards, predict the segmentation of a sample using the fitted model. # Training the model with 80 samples for 500 epochs sample_list = data_io.get_indiceslist() model.train(sample_list[30:120], epochs=500) # Predict the segmentation of 20 samples model.predict(sample_list[0:30]) The predicted segmentation results will be passed to the Data I/O interface with the function save_prediction. The already implemented interfaces (like NiFTI interface) will store the predictions into files. 4) Results We used our fitted U-Net model and successfully predicted a segmentation for our Kidney Tumor CT image. But how do we know, how good our segmentation is compared to the ground-truth? We have the possibility to calculate several different scores (like the already implemented variants of the Dice Similarity Coefficient), but for simplicity, we want to compare it manually by eye. MIScnn provides a simple function to create comparison images (and for 3D data GIFs). # Load the sample sample = data_io.sample_loader(sample_list[24], load_seg=True, load_pred=True) # Access image, truth and predicted segmentation data img, seg, pred = sample.img_data, sample.seg_data, sample.pred_data # Visualize the truth and prediction segmentation as a GIF from miscnn.utils.visualizer import visualize_evaluation visualize_evaluation(sample_list[24], img, seg, pred, \"plot_directory/\") Complete Code import miscnn # Initialize the NIfTI I/O interface and configure the images as one channel (grayscale) # and three segmentation classes (background, kidney, tumor) from miscnn.data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) # Create the Data I/O object data_path = \"/home/mudomini/projects/kits19/data/\" data_io = miscnn.Data_IO(interface, data_path) # Create and configure the Data Augmentation class data_aug = miscnn.Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True, brightness=True, contrast=True, gamma=True, gaussian_noise=True) # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, data_aug=data_aug, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160)) # Import standard U-Net architecture and Soft Dice from miscnn.neural_network.architecture.unet.standard import Architecture from miscnn.neural_network.metrics import dice_soft unet_standard = Architecture() # Create a deep learning neural network model with a standard U-Net architecture model = miscnn.Neural_Network(preprocessor=pp, architecture=unet_standard, loss=dice_soft) # Training the model with 80 samples for 500 epochs sample_list = data_io.get_indiceslist() model.train(sample_list[0:80], epochs=500) # Predict the segmentation of 20 samples model.predict(sample_list[80:100]) # Load the sample sample = data_io.sample_loader(sample_list[80], load_seg=True, load_pred=True) # Access image, truth and predicted segmentation data img, seg, pred = sample.img_data, sample.seg_data, sample.pred_data # Visualize the truth and prediction segmentation as a GIF from miscnn.utils.visualizer import visualize_evaluation visualize_evaluation(sample_list[80], img, seg, pred, \"plot_directory/\") For more detailed coding examples, check out the MIScnn wiki for tutorials or examples .","title":"Basic Usage"},{"location":"Usage/#core-steps-of-the-miscnn-pipeline","text":"The MIScnn pipeline is represented by 4 core classes: Data I/O Data Augmentation (optional) Preprocessor Neural Network Model These classes handle all required steps for medical image segmentation and can be extensively customized. All classes, except the Data Augmentation class, use switchable interfaces which results into high configurability and offers simple integration of user-defined solutions.","title":"Core steps of the MIScnn pipeline"},{"location":"Usage/#workflow-of-miscnn","text":"","title":"Workflow of MIScnn"},{"location":"Usage/#import-the-miscnn-module","text":"Before constructing our Medical Image Segmentation pipeline, we are going to import the MIScnn modules which will provide our core classes and functions. import miscnn","title":"Import the MIScnn Module"},{"location":"Usage/#1-data-io","text":"The first step in the MIScnn pipeline is to establish the Data I/O class. The Data I/O handles the complete data loading of biomedical images, its segmentation and storage of predicted files, as well as temporary files like batches. In order to open the MIScnn pipeline for a various data formats and structures, the data I/O class uses switchable I/O interfaces. It is possible to use already provided I/O interfaces with a fixed data structure or custom I/O interfaces for integration of your specific data structure into the MIScnn pipeline. Before initializing an instance of the Data I/O class, we have to select an I/O interface. This interface have to load our data set into the MIScnn pipeline. In our example, we are using the KiTS19 data set which are kidney tumor CT scans encoded in the NIfTI format. Therefore, we call the provided NIfTI interface with a single channel (grayscale image) and 3 classes for the segmentation (background, kidney and tumor). from miscnn.data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) Afterwards, we can initialize the Data I/O class by passing our newly created interface and the path to the data set to the constructor. data_path = \"/home/mudomini/projects/kits19/data/\" data_io = miscnn.Data_IO(interface, data_path)","title":"1) Data I/O"},{"location":"Usage/#2-preprocessor","text":"Finally, we can initialize the Preprocessor class. Therefore, we are passing our Data I/O Interface to the Preprocessor initialization. Additionally, we configure our Preprocessor to create batches on-the-fly (preprare_batches=False) by random cropping patches with size 80x160x160 out of the image. Also a batch should contain 2 patches. # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160))","title":"2) Preprocessor"},{"location":"Usage/#specefic-data-augmentation","text":"If you want to specify the Data Augmentation, it is possible to configure the Data Augmentation class. A Preprocessor object with default parameters automatically initialize a Data Augmentation class with default values, but here we initialize it by hand to illustrate the exact workflow of the MIScnn pipeline. The parameters for the Data Augmentation configure which augmentation techniques should be applied to the data set. In this case, we are using all possible augmentation techniques in order to run extensive data augmentation and avoid overfitting # Create and configure the Data Augmentation class data_aug = miscnn.Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True, brightness=True, contrast=True, gamma=True, gaussian_noise=True) # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, data_aug=data_aug, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160))","title":"Specefic Data Augmentation"},{"location":"Usage/#3-neural-network-model","text":"Now it's time to initialize our final object: The Neural Network. With the Neural Network class, we are able to run training and prediction operations. But before, we have to decide which Neural Network Architecture and Metric we want to use. In order to show the simplicity and performance of MIScnn, we stick with the simple 3D U-Net Architecture for our Neural Network without any special tricks or optimizations. For training, we are using the Soft Dice as loss. # Import standard U-Net architecture and Soft Dice from miscnn.neural_network.architecture.unet.standard import Architecture from miscnn.neural_network.metrics import dice_soft unet_standard = Architecture() # Create a deep learning neural network model with a standard U-Net architecture model = miscnn.Neural_Network(preprocessor=pp, architecture=unet_standard, loss=dice_soft) Now, let's run a model training on our data set for 500 epochs and, afterwards, predict the segmentation of a sample using the fitted model. # Training the model with 80 samples for 500 epochs sample_list = data_io.get_indiceslist() model.train(sample_list[30:120], epochs=500) # Predict the segmentation of 20 samples model.predict(sample_list[0:30]) The predicted segmentation results will be passed to the Data I/O interface with the function save_prediction. The already implemented interfaces (like NiFTI interface) will store the predictions into files.","title":"3) Neural Network Model"},{"location":"Usage/#4-results","text":"We used our fitted U-Net model and successfully predicted a segmentation for our Kidney Tumor CT image. But how do we know, how good our segmentation is compared to the ground-truth? We have the possibility to calculate several different scores (like the already implemented variants of the Dice Similarity Coefficient), but for simplicity, we want to compare it manually by eye. MIScnn provides a simple function to create comparison images (and for 3D data GIFs). # Load the sample sample = data_io.sample_loader(sample_list[24], load_seg=True, load_pred=True) # Access image, truth and predicted segmentation data img, seg, pred = sample.img_data, sample.seg_data, sample.pred_data # Visualize the truth and prediction segmentation as a GIF from miscnn.utils.visualizer import visualize_evaluation visualize_evaluation(sample_list[24], img, seg, pred, \"plot_directory/\")","title":"4) Results"},{"location":"Usage/#complete-code","text":"import miscnn # Initialize the NIfTI I/O interface and configure the images as one channel (grayscale) # and three segmentation classes (background, kidney, tumor) from miscnn.data_loading.interfaces.nifti_io import NIFTI_interface interface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3) # Create the Data I/O object data_path = \"/home/mudomini/projects/kits19/data/\" data_io = miscnn.Data_IO(interface, data_path) # Create and configure the Data Augmentation class data_aug = miscnn.Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True, brightness=True, contrast=True, gamma=True, gaussian_noise=True) # Create and configure the Preprocessor class pp = miscnn.Preprocessor(data_io, data_aug=data_aug, batch_size=2, analysis=\"patchwise-crop\", patch_shape=(80,160,160)) # Import standard U-Net architecture and Soft Dice from miscnn.neural_network.architecture.unet.standard import Architecture from miscnn.neural_network.metrics import dice_soft unet_standard = Architecture() # Create a deep learning neural network model with a standard U-Net architecture model = miscnn.Neural_Network(preprocessor=pp, architecture=unet_standard, loss=dice_soft) # Training the model with 80 samples for 500 epochs sample_list = data_io.get_indiceslist() model.train(sample_list[0:80], epochs=500) # Predict the segmentation of 20 samples model.predict(sample_list[80:100]) # Load the sample sample = data_io.sample_loader(sample_list[80], load_seg=True, load_pred=True) # Access image, truth and predicted segmentation data img, seg, pred = sample.img_data, sample.seg_data, sample.pred_data # Visualize the truth and prediction segmentation as a GIF from miscnn.utils.visualizer import visualize_evaluation visualize_evaluation(sample_list[80], img, seg, pred, \"plot_directory/\") For more detailed coding examples, check out the MIScnn wiki for tutorials or examples .","title":"Complete Code"},{"location":"Validation/","text":"The validation is one of the key steps in any medical image analysis. The aim of the validation is to utilize the fitted model in order to provide an unbiased performance evaluation of it. The resulting validation performance can be an indicator for overfitting to the training data or the need of hyperparameter adjustment. MIScnn provides the following three popular validation techniques: Cross-Validation, Percentage-Split Validation and Leave-One-Out Validation. Methods k-fold Cross-Validation cross_validation(sample_list, model, k_fold=3, epochs=20, iterations=None, evaluation_path=\"evaluation\", draw_figures=True, run_detailed_evaluation=True, callbacks=[], save_models=True, direct_output=False) Function for an automatic k-fold Cross-Validation of the Neural Network model by running the whole pipeline several times with different data set combinations. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - k_fold: The number of k-folds for the Cross-Validation. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. - draw_figures: Boolean, whether evaluation figures should be automatically plotted in the evaluation directory. - run_detailed_evaluation: Boolean, whether a detailed evaluation (additional prediction) should be performed. - callbacks: A list of Callback classes for custom evaluation. - save_models: Boolean, whether fitted models should be stored or thrown away. - direct_output: Boolean, whether computed evaluations will be output as the return of this function or if the evaluations will be saved on disk in the evaluation directory. Returns: None or validation results if direct_output is true. The variable validation_results is a list of Keras History objects containing all kinds of information. For each fold, a history object will created and appended to the list. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import cross_validation cross_validation(sample_list, model, k_fold=3, epochs=50) Percentage-Split Validation split_validation(sample_list, model, percentage=0.2, epochs=20, iterations=None, evaluation_path=\"evaluation\", draw_figures=True, run_detailed_evaluation=True, callbacks=[], direct_output=False) Function for an automatic Percentage-Split Validation of the Neural Network model by running the whole pipeline once with a test and train data set. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - percentage: Testing set percentage of the whole sample list size. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. - draw_figures: Boolean, whether evaluation figures should be automatically plotted in the evaluation directory. - run_detailed_evaluation: Boolean, whether a detailed evaluation (additional prediction) should be performed. - callbacks: A list of Callback classes for custom evaluation. - direct_output: Boolean, whether computed evaluations will be output as the return of this function or if the evaluations will be saved on disk in the evaluation directory. Returns: None or a Keras History objects if direct_output is true. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import split_validation split_validation(sample_list, model, percentage=0.2, epochs=50) Leave-One-Out Validation leave_one_out(sample_list, model, epochs=20, iterations=None, callbacks=[], evaluation_path=\"evaluation\") Function for an automatic Leave-One-Out Validation of the Neural Network model by running the whole pipeline once by training on the complete data set except one sample and then predict the segmentation of the last remaining sample. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Callback classes for custom evaluation. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. Returns: None. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import detailed_validation detailed_validation(sample_list, model, evaluation_path=\"evaluation\") Detailed Validation detailed_validation(validation_samples, model, evaluation_path) Function for detailed validation of a validation sample data set. The segmentation of these samples will be predicted with an already fitted model and evaluated. Normally, this method will be called automatically inside one of the three validation techniques. Nevertheless, it is possible to use the detailed validation method manually. Be aware, that this function only predicts and evaluates the given samples. The detailed validation does NOT include any training process. Arguments: - validation_samples: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. Returns: None. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation.detailed_validation import detailed_validation detailed_validation(validation_samples, model, evaluation_path=\"evaluation\")","title":"Validation Techniques"},{"location":"Validation/#methods","text":"","title":"Methods"},{"location":"Validation/#k-fold-cross-validation","text":"cross_validation(sample_list, model, k_fold=3, epochs=20, iterations=None, evaluation_path=\"evaluation\", draw_figures=True, run_detailed_evaluation=True, callbacks=[], save_models=True, direct_output=False) Function for an automatic k-fold Cross-Validation of the Neural Network model by running the whole pipeline several times with different data set combinations. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - k_fold: The number of k-folds for the Cross-Validation. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. - draw_figures: Boolean, whether evaluation figures should be automatically plotted in the evaluation directory. - run_detailed_evaluation: Boolean, whether a detailed evaluation (additional prediction) should be performed. - callbacks: A list of Callback classes for custom evaluation. - save_models: Boolean, whether fitted models should be stored or thrown away. - direct_output: Boolean, whether computed evaluations will be output as the return of this function or if the evaluations will be saved on disk in the evaluation directory. Returns: None or validation results if direct_output is true. The variable validation_results is a list of Keras History objects containing all kinds of information. For each fold, a history object will created and appended to the list. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import cross_validation cross_validation(sample_list, model, k_fold=3, epochs=50)","title":"k-fold Cross-Validation"},{"location":"Validation/#percentage-split-validation","text":"split_validation(sample_list, model, percentage=0.2, epochs=20, iterations=None, evaluation_path=\"evaluation\", draw_figures=True, run_detailed_evaluation=True, callbacks=[], direct_output=False) Function for an automatic Percentage-Split Validation of the Neural Network model by running the whole pipeline once with a test and train data set. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - percentage: Testing set percentage of the whole sample list size. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. - draw_figures: Boolean, whether evaluation figures should be automatically plotted in the evaluation directory. - run_detailed_evaluation: Boolean, whether a detailed evaluation (additional prediction) should be performed. - callbacks: A list of Callback classes for custom evaluation. - direct_output: Boolean, whether computed evaluations will be output as the return of this function or if the evaluations will be saved on disk in the evaluation directory. Returns: None or a Keras History objects if direct_output is true. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import split_validation split_validation(sample_list, model, percentage=0.2, epochs=50)","title":"Percentage-Split Validation"},{"location":"Validation/#leave-one-out-validation","text":"leave_one_out(sample_list, model, epochs=20, iterations=None, callbacks=[], evaluation_path=\"evaluation\") Function for an automatic Leave-One-Out Validation of the Neural Network model by running the whole pipeline once by training on the complete data set except one sample and then predict the segmentation of the last remaining sample. Arguments: - sample_list: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - epochs: Number of epochs. A single epoch is defined as one iteration through the complete data set. - iterations: Number of iterations (batches) in a single epoch. - callbacks: A list of Callback classes for custom evaluation. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. Returns: None. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation import detailed_validation detailed_validation(sample_list, model, evaluation_path=\"evaluation\")","title":"Leave-One-Out Validation"},{"location":"Validation/#detailed-validation","text":"detailed_validation(validation_samples, model, evaluation_path) Function for detailed validation of a validation sample data set. The segmentation of these samples will be predicted with an already fitted model and evaluated. Normally, this method will be called automatically inside one of the three validation techniques. Nevertheless, it is possible to use the detailed validation method manually. Be aware, that this function only predicts and evaluates the given samples. The detailed validation does NOT include any training process. Arguments: - validation_samples: A list of sample indicies which will be used for validation. - model: Instance of a Neural Network model class instance. - evaluation_path: Path to the evaluation data directory. This directory will be created and used for storing all kinds of evaluation results during the validation processes. Returns: None. Example: model = miscnn.Neural_Network(preprocessor=pp) from miscnn.evaluation.detailed_validation import detailed_validation detailed_validation(validation_samples, model, evaluation_path=\"evaluation\")","title":"Detailed Validation"}]}